{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Setup and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim import models\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "pd.options.mode.chained_assignment = None\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataframe_raw.temp.csv').drop(columns = ['Unnamed: 0'])\n",
    "df['headline'] = df['headline'].str.replace('[^a-zA-Z ]', '')\n",
    "df['headline'] = df['headline'].str.lower()\n",
    "df = df.fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Embedding Headline Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Constructing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headlines = np.asarray([sentence.split() for sentence in df.headline])\n",
    "\n",
    "headlines_model = gensim.models.Word2Vec(\n",
    "    tokenized_headlines, size = 300, window = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_word_to_vector = dict(\n",
    "    zip(headlines_model.wv.index2word, headlines_model.wv.vectors)\n",
    ")\n",
    "\n",
    "headlines_vector_to_word = dict(\n",
    "    zip([tuple(word) for word in headlines_model.wv.vectors.tolist()],\n",
    "        headlines_model.wv.index2word)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Intuitive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words:\n",
      "word 1\t\tword 2\t\tsimilarity\n",
      "'trump'\t\t'obama'\t\t0.78\n",
      "'cat'\t\t'dog'\t\t0.88\n",
      "'peace'\t\t'war'\t\t0.58\n",
      "\n",
      "Dissimilar words:\n",
      "word 1\t\tword 2\t\tsimilarity\n",
      "'trump'\t\t'dog'\t\t0.25\n",
      "'cat'\t\t'war'\t\t-0.04\n",
      "'peace'\t\t'obama'\t\t0.39\n"
     ]
    }
   ],
   "source": [
    "similar_words = [('trump', 'obama'), ('cat', 'dog'), ('peace', 'war')]\n",
    "dissimilar_words = [('trump', 'dog'), ('cat', 'war'), ('peace', 'obama')]\n",
    "\n",
    "print('Similar words:\\nword 1\\t\\tword 2\\t\\tsimilarity')\n",
    "for word_a, word__b in similar_words:\n",
    "    print('%r\\t\\t%r\\t\\t%.2f' % (word_a, word__b, headlines_model.wv.similarity(word_a, word__b)))\n",
    "\n",
    "print('\\nDissimilar words:\\nword 1\\t\\tword 2\\t\\tsimilarity')\n",
    "for word_a, word__b in dissimilar_words:\n",
    "    print('%r\\t\\t%r\\t\\t%.2f' % (word_a, word__b, headlines_model.wv.similarity(word_a, word__b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trumps', 'obama', 'trumpus', 'glover', 'ted', 'congress', 'sanders', 'gop', 'marco', 'roy']\n",
      "['toddler', 'dog', 'puppy', 'grandma', 'robot', 'baby', 'butt', 'pants', 'twins', 'selfie']\n",
      "['iraq', 'racism', 'terrorism', 'freedom', 'movement', 'democracy', 'rights', 'humanity', 'leadership', 'politics']\n",
      "['lie', 'word', 'tweet', 'answer', 'joke', 'truth', 'theory', 'thing', 'thinking', 'lies']\n"
     ]
    }
   ],
   "source": [
    "random_words_list = ['trump', 'cat', 'war', 'question']\n",
    "for word in random_words_list:\n",
    "    print([tup[0] for tup in headlines_model.wv.most_similar(positive=[word], topn=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head \tdoes not fit in\t ['data', 'science', 'maths', 'statistics', 'head']\n",
      "food \tdoes not fit in\t ['obama', 'trump', 'hillary', 'sanders', 'food']\n",
      "eat \tdoes not fit in\t ['cat', 'dog', 'pet', 'eat']\n"
     ]
    }
   ],
   "source": [
    "similar_words_except_one = [\n",
    "    ['data', 'science', 'maths', 'statistics', 'head'],\n",
    "    ['obama', 'trump', 'hillary', 'sanders', 'food'],\n",
    "    ['cat', 'dog', 'pet', 'eat']\n",
    "]\n",
    "\n",
    "for word_list in similar_words_except_one:\n",
    "    print(headlines_model.wv.doesnt_match(word_list), '\\tdoes not fit in\\t', word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf():\n",
    "    \n",
    "    grouped_by_category = df.groupby('category').headline.apply(list)\n",
    "    categories = grouped_by_category.index.tolist()\n",
    "    grouped = [' '.join(category) for category in grouped_by_category]\n",
    "\n",
    "    all_words_set = set(' '.join(df['headline']).split(' '))\n",
    "    all_words_list = list(all_words_set)\n",
    "    all_words_dict = dict(zip(all_words_list, list(range(len(all_words_list)))))\n",
    "    \n",
    "    word_count_per_category_dict = {}\n",
    "    for category in categories:\n",
    "        word_count_per_category_dict[category] = np.zeros(len(all_words_list))\n",
    "        for tup in Counter(grouped[categories.index(category)].split(' ')).most_common():\n",
    "            word_count_per_category_dict[category][all_words_dict[tup[0]]] += tup[1]\n",
    "            \n",
    "    word_count_per_category_df = pd.DataFrame.from_dict(word_count_per_category_dict)\n",
    "    \n",
    "    category_word_count = {}\n",
    "    for category in categories:\n",
    "        category_word_count[category] = len(\n",
    "            ' '.join(df[df['category'] == category]['headline']).split(' ')\n",
    "        )\n",
    "\n",
    "    tf = {}\n",
    "    for cat in categories:\n",
    "        tf[cat] = [\n",
    "            w / category_word_count[cat] for w in word_count_per_category_dict[cat]\n",
    "        ]\n",
    "    \n",
    "    idf = []\n",
    "    for index, row in word_count_per_category_df.iterrows():\n",
    "        idf.append(np.log(len(categories) / np.count_nonzero(row)))\n",
    "        \n",
    "    tfidf = {}\n",
    "    for category in categories:\n",
    "        tfidf[category] = [tf * idf for tf, idf in zip(tf[category], idf)]\n",
    "\n",
    "    return tfidf, all_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_tf_idf, headlines_word_list = calculate_tf_idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Headlines embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVector(word, model):\n",
    "    if word in model.wv.index2word:\n",
    "        return model.wv[word]\n",
    "\n",
    "def vectorToWord(vector, model, dictionary):\n",
    "    if vector in [tuple(word) for word in model.wv.vectors.tolist()]:\n",
    "        return dictionary[tuple(vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_headlines = [\n",
    "    [wordToVector(w, headlines_model) for w in h] for h in tokenized_headlines\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Not weighing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(model, tokens):\n",
    "\n",
    "    vectorized = [[wordToVector(word, model) for word in t] for t in tokens]\n",
    "\n",
    "    vectors_means = [\n",
    "        np.mean([v for v in t if v is not None], axis=0) for t in vectorized\n",
    "    ]\n",
    "    \n",
    "    return vectors_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_vectors_means = get_mean_vector(headlines_model, tokenized_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Weighing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_words_with_tf_idf(vectorized_words, tfidf, all_words_list, tokenized_texts):\n",
    "    all_words_dict = dict(zip(all_words_list, list(range(len(all_words_list)))))\n",
    "    weighted = list(vectorized_words)\n",
    "    for text in range(df.shape[0]):\n",
    "        category = df.loc[text].category\n",
    "        for word in range(len(weighted[text])):\n",
    "            word_index = all_words_dict[tokenized_texts[text][word]]\n",
    "            if type(weighted[text][word]) == np.ndarray:\n",
    "                weighted[text][word] = weighted[text][word] * tfidf[category][word_index]\n",
    "\n",
    "    return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_headlines_weighted = weigh_words_with_tf_idf(\n",
    "    list(vectorized_headlines), headlines_tf_idf, headlines_word_list, tokenized_headlines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_vectors_means_weighted = [\n",
    "    np.mean([\n",
    "        vector for vector in headline if vector is not None\n",
    "    ], axis=0) for headline in vectorized_headlines_weighted\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Formatting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_mean = df.category.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_vectors_means):\n",
    "    means = [vector.tolist() for vector in text_vectors_means]\n",
    "    text_nans = np.where([vector != vector for vector in means])[0]\n",
    "    means_clean = np.delete(np.asarray(means), text_nans)\n",
    "    categories_clean = np.delete(np.asarray(categories_mean), text_nans)\n",
    "    return means_clean, categories_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_mean_clean, headlines_categories_clean = clean(headline_vectors_means)\n",
    "\n",
    "headlines_weighted_mean_clean, headlines_w_categories_clean = clean(\n",
    "    headline_vectors_means_weighted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Separating predictor and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(name, X_Y):\n",
    "    X_df = pd.DataFrame({name + '_vectors' : X_Y[0]})\n",
    "    X_Y_df = pd.DataFrame(X_df[name + '_vectors'].values.tolist())\n",
    "    X_Y_df['categories'] = X_Y[1]\n",
    "    X = X_Y_df.drop(columns = ['categories'], inplace = False)\n",
    "    Y = X_Y_df.categories\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_X, headlines_Y = split(\n",
    "    'headlines', (headlines_mean_clean, headlines_categories_clean)\n",
    ")\n",
    "\n",
    "headlines_weighted_X, headlines_weighted_Y = split(\n",
    "    'headlines_weighted', (headlines_weighted_mean_clean, headlines_w_categories_clean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Separating train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    headlines_X, headlines_Y, test_size = 0.2, random_state = 0\n",
    ")\n",
    "\n",
    "X_train_weighted, X_test_weighted, Y_train_weighted, Y_test_weighted = train_test_split(\n",
    "    headlines_weighted_X, headlines_weighted_Y, test_size = 0.2, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Saving train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sets = [\n",
    "    ['X_train', X_train], ['X_test', X_test],\n",
    "    ['Y_train', Y_train], ['Y_test', Y_test],\n",
    "    ['X_train_weighted', X_train_weighted], ['X_test_weighted', X_test_weighted],\n",
    "    ['Y_train_weighted', Y_train_weighted], ['Y_test_weighted', Y_test_weighted],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_set in all_data_sets:\n",
    "    data_set[1].to_csv('train_and_test_sets/' + data_set[0] +  '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
