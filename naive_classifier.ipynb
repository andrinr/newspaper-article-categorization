{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Naive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataframe_raw.temp.csv').drop(columns = ['Unnamed: 0'])\n",
    "df['headline'] = df['headline'].str.replace('[^a-zA-Z ]', '')\n",
    "df['headline'] = df['headline'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:42<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "popular_words = pd.DataFrame(columns = ['category', 'words'])\n",
    "for category in tqdm(set(df.category), position = 0):\n",
    "    category_headlines = [\n",
    "        h for h in df[df['category'] == category].headline.tolist() if type(h) == str\n",
    "    ]\n",
    "    category_words_by_frequency = Counter(''.join(category_headlines).split(' ')).most_common()\n",
    "    all_words = [s for s in [tup[0] for tup in category_words_by_frequency] if s]\n",
    "\n",
    "    words = [\n",
    "        word for (word, pos) in nltk.pos_tag(all_words) if pos[:2] in ['NN', 'JJ', 'VB']\n",
    "    ]\n",
    "    \n",
    "    popular_words = popular_words.append(\n",
    "        {'category' : category, 'words' : words}, ignore_index = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_english_words_dict = nltk.FreqDist(nltk.corpus.brown.words()).most_common()[:1000]\n",
    "most_common_words = [tup[0].lower() for tup in most_common_english_words_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_words['corpus_specific_words'] = [\n",
    "    [w for w in s if w not in most_common_words] for s in popular_words.words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting the most common category would yield an accuracy of about 16.3 %.\n"
     ]
    }
   ],
   "source": [
    "categories_cnt = list(Counter(df.category).most_common())\n",
    "benchmark = round(\n",
    "    max([obs[1] for obs in categories_cnt]) / sum([obs[1] for obs in categories_cnt]), 3\n",
    ")\n",
    "print('Always predicting the most common category would yield an accuracy of about',\n",
    "      benchmark * 100, '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest word list has 2365 words.\n"
     ]
    }
   ],
   "source": [
    "min_len = min([len(words) for words in popular_words.corpus_specific_words])\n",
    "print('The smallest word list has', min_len, 'words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Naive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_classifier(sample_size, nr_words, reward_strength):\n",
    "    actual_categories, predicted_categories = [], []\n",
    "    correct_predictions, false_predictions = 0, 0\n",
    "\n",
    "    for index, row in df.sample(sample_size).iterrows():\n",
    "        correct_category, best_category = row.category, [0, 0]\n",
    "    \n",
    "        for category in set(df.category):\n",
    "            top_words = list(\n",
    "                popular_words.loc[popular_words['category'] == category].\\\n",
    "                corpus_specific_words.tolist()[0])[:nr_words]\n",
    "            \n",
    "            reward_score = 0\n",
    "            length = len(row.headline.split(' ')) if type(row.headline) == str else 100\n",
    "\n",
    "            if type(row.headline) == str:\n",
    "                for word in row.headline.split(' '):\n",
    "                    if word in top_words:\n",
    "                        reward_score += (nr_words - top_words.index(word)) ** reward_strength\n",
    "            \n",
    "            reward_score /= length\n",
    "            if len(best_category) == 0: best_category = [category, reward_score]\n",
    "            if reward_score > best_category[1]: best_category = [category, reward_score]\n",
    "                \n",
    "        if best_category[0] == correct_category: correct_predictions += 1\n",
    "        else: false_predictions += 1\n",
    "        actual_categories.append(correct_category)\n",
    "        predicted_categories.append(best_category[0])\n",
    "\n",
    "    accuracy = round(correct_predictions / sample_size * 100, 2)\n",
    "    actual_predicted = list(zip(actual_categories, predicted_categories))\n",
    "    \n",
    "    return (accuracy, actual_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(sample_size, nrs_words, reward_strengths):\n",
    "    \n",
    "    classifier_evaluation = {}\n",
    "    \n",
    "    for params in tqdm(list(itertools.product(nrs_words, reward_strengths)), position = 0):\n",
    "        accuracy, confusion = naive_classifier(sample_size, params[0], params[1])\n",
    "        classifier_evaluation[(params[0], params[1])] = (accuracy, confusion)\n",
    "\n",
    "    return sorted(classifier_evaluation.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best classifier has a 49.7 % accuracy.\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_classifier(1000, [20, 200, 2000], [0.5, 1, 2])\n",
    "best_accuracy = [(tup[0], tup[1][0]) for tup in results][0][1]\n",
    "print('The best classifier has a', best_accuracy, '% accuracy.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
